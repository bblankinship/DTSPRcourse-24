{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f29f8b6",
   "metadata": {},
   "source": [
    "# Tutorial 5 Python Notebook \n",
    "\n",
    "In this tutorial we will return to the [Public Health Scotland around Stroke Activity](https://www.opendata.nhs.scot/dataset/scottish-stroke-statistics/resource/47656572-e196-40c8-83e8-08b0b223b2e6) dataset that we used in last week's tutorial. We will also be using the [Health Board Labels](https://www.opendata.nhs.scot/dataset/geography-codes-and-labels/resource/652ff726-e676-4a20-abda-435b98dd7bdc) dataset. \n",
    "\n",
    "The aim of this tutorial is to give you some (guided) hands-on experience joining and reshaping data frames, as well as to reinforce some of the learning we have done across the course. There are 8 tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages and modules \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9beb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data sets \n",
    "\n",
    "stroke_raw = pd.read_csv(\"https://www.opendata.nhs.scot/dataset/f5dcf382-e6ca-49f6-b807-4f9cc29555bc/resource/47656572-e196-40c8-83e8-08b0b223b2e6/download/stroke_activitybyhbr.csv\")\n",
    "\n",
    "hb = pd.read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34a4a1",
   "metadata": {},
   "source": [
    "It is always a good idea to quickly double check your data has been read in as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99c1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b464da9",
   "metadata": {},
   "source": [
    "Below is an image showing where the different Health Boards are in Scotland on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faddc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"../figures/Map_of_Health_Boards.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd52ee",
   "metadata": {},
   "source": [
    "## Question to solve \n",
    "\n",
    "The question we are trying to answer with the data is: \n",
    "\n",
    "> What is the average number of discharges with a stroke diagnosis by age group in the East region of Scotland for all admissions in the finanical year 2019/20 and 2020/21?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470788d7",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Looking at these two data frames, what columns do you think are the linkage keys? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62964076",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3644f",
   "metadata": {},
   "source": [
    "#### Task 1 Solution \n",
    "\n",
    "In the `stroke_raw` dataset there is `HBR` (health board of residence), which links to the `HB` column in the `hb` dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c843028",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Stroke activity dataset with the [Health Board Labels](https://www.opendata.nhs.scot/dataset/geography-codes-and-labels/resource/652ff726-e676-4a20-abda-435b98dd7bdc) dataset into a new data frame called `stroke_join`. \n",
    "\n",
    "In the last above we identified the linkage key variable(s), which is the first step when wanting to complete a join. Next, you need to decide on the type of join you want to use and then implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aeb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f1259",
   "metadata": {},
   "source": [
    "#### Task 2 Solution \n",
    "\n",
    "Either a left join or full join would be appropriate. An inner join is not appropriate here as it removes 2880 observations from the `stroke_raw` data set. In this example solution, I have used a left join, with the `stroke_raw` data set on the left and the `hb` data set on the right (keeping all data from `stroke_raw` and only matching data from `hb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b8661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stroke_join = pd.merge(stroke_raw, hb, \n",
    "                       how = \"left\", \n",
    "                       left_on = \"HBR\", \n",
    "                       right_on = \"HB\")\n",
    "\n",
    "stroke_join # has 43200 rows and 20 columns - this solution keeps both HBR and HB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another solution keeping only 1 likage key column in the resulting df \n",
    "## is to rename HBR in stroke_raw first before the merge \n",
    "\n",
    "stroke_raw.rename({\"HBR\": \"HB\"}, axis = 1).merge(hb, how = \"left\", on = \"HB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911b917",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "To answer our question outlined above, we do not need all of the columns currently in the `stroke_join` dataset. Process the data to include only the variables needed to answer the question and save this processed dataset into an object called `stroke`.\n",
    "\n",
    "Check the dtypes of the remaining columns and cast them if not appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550624f",
   "metadata": {},
   "source": [
    "<details><summary style='color:darkblue'>HINT: Beware of surprise summary or aggregate data! CLICK HERE TO SEE MORE.</summary>\n",
    "\n",
    "Beware of aggregate or summary level data, even in variables not needed to directly answer the question. Consulting the data dictionary (if provided) or doing data checks is crucial at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53bc3d",
   "metadata": {},
   "source": [
    "#### Task 3 solution \n",
    "\n",
    "The variables we need to answer the question are `FinancialYear`, `AdmissionType`, `AgeGroup`, `Diagnosis`, `NumberOfDischarges`, and `HBName`. We also need `Sex` as we need to filter the data to include only the aggregate level `All` to avoid duplicate data, although `Sex` is not directly related to our question of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d628d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stroke = stroke_join.loc[:,[\"FinancialYear\", \"AdmissionType\", \"AgeGroup\", \n",
    "                            \"Sex\", \"Diagnosis\", \"NumberOfDischarges\", \"HBName\"]]\n",
    "\n",
    "stroke # excellent - now we have only 7 columns but retain all 43200 rows of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8597e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stroke.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c37335",
   "metadata": {},
   "source": [
    "The variables which should be categorical are currently of dtype object. So let's convert them. We can do this elengantly with a list data structure input, instead of writing repeating lines of code for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b542ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"FinancialYear\", \"AdmissionType\", \"AgeGroup\", \"Sex\", \n",
    "            \"Diagnosis\", \"HBName\"]\n",
    "\n",
    "stroke[cat_cols] = stroke[cat_cols].astype(\"category\")\n",
    "\n",
    "# you could put the list directly into the code instead of creating an object\n",
    "## but the code is a bit easier to read if you create an object first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.dtypes # all ready to go! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb391b5",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "What is the shape of the `stroke` data currently? Is it in a suitable shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ae764",
   "metadata": {},
   "source": [
    "#### Task 4 Solution \n",
    "\n",
    "The `stroke` data frame is currently in long format as we have a single column for each variable, which is indeed what we want. Long format makes it easier to manipulate and wrangle data. So we will keep it that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f30785",
   "metadata": {},
   "source": [
    "### Task 5 \n",
    "\n",
    "Now that we have our joined data set, it is important to inspect the data for any missing or aggregate values. We know from last week that this data set has many aggregate level responses! Check for the unique values of all 7 variables in `stroke`. Are there any unexpected findings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa89508",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6351b",
   "metadata": {},
   "source": [
    "#### Task 5 Solution \n",
    "\n",
    "We know from last week that `AgeGroup` and `AdmissionType` each include aggregate level responses. For completeness sake I have included the code exploring these variables again here as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf543f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stroke.describe(include = \"all\") # by default only numeric are included unless you specific to include all "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42322fc2",
   "metadata": {},
   "source": [
    "The describe output does not give us details about what the categories include, so we will need to take a closer look at each variable. It also looks like there are some missing values in the `NumberOfDischarges` column based on the count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check FinanicalYear \n",
    "\n",
    "print(stroke.FinancialYear.unique()) # no aggregate responses \n",
    "\n",
    "print(stroke.FinancialYear.isna().sum()) # no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check AdmissionType \n",
    "\n",
    "print(stroke.AdmissionType.unique()) # aggregate response All \n",
    "\n",
    "print(stroke.AdmissionType.isna().sum()) # no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check AgeGroup  \n",
    "\n",
    "print(stroke.AgeGroup.unique()) # aggregate responses under75 years & All \n",
    "\n",
    "print(stroke.AgeGroup.isna().sum()) # no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check Sex \n",
    "\n",
    "print(stroke.Sex.unique()) # aggregate responses All - which is what we want to keep \n",
    "\n",
    "print(stroke.Sex.isna().sum()) # no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Diagnosis \n",
    "\n",
    "print(stroke.Diagnosis.unique()) # no aggregate responses \n",
    "\n",
    "print(stroke.Diagnosis.isna().sum()) # no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NumberofDischarges \n",
    "\n",
    "print(stroke.NumberOfDischarges.describe()) # values range from 0 to 35404 \n",
    "# count only 39503 of the total 43200 rows suggesting some NA values \n",
    "\n",
    "## we can look for the total number of NaN values using isna and then sum (Truthy and Falsey are helpful here!)\n",
    "print(stroke.NumberOfDischarges.isna().sum())\n",
    "\n",
    "## or we can confirm that there are ANY NaN values at all in the column \n",
    "print(stroke.NumberOfDischarges.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check HBName \n",
    "\n",
    "print(stroke.HBName.unique()) # we have some nan values! There are 14 HBs as expected but what could this nan be?\n",
    "# well, we know from last week that HBR in the stroke dataset included all of Scotland with the country code S92000003\n",
    "\n",
    "# lets see how many missing data points there are \n",
    "print(stroke.HBName.isna().sum()) # 2880 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fca636",
   "metadata": {},
   "source": [
    "Because we still have the unedited `stroke_raw` data, we can check if these missing values match up to the aggregate country level. Having a raw data frame version is super useful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_raw.loc[stroke_raw[\"HBR\"] == \"S92000003\"] \n",
    "\n",
    "# ah ha! Indeed there are 2880 observations at the country code level in the raw dataset. Mystery solved!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de6331",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "We now know there are both aggregate level responses in our data frame as well as missing data. Before we deal with any missing data unnecessarily, let's filter out the responses we are not interested in (i.e., remove the rows we do not need to answer the question) and then check again for any missing data. It is likely that in doing so, the missing data may not be a problem anymore. \n",
    "\n",
    "Save your filtered data into a dataframe called `stroke_q`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3822552",
   "metadata": {},
   "source": [
    "<details><summary style='color:darkblue'>HINT: Breaking down the question. CLICK HERE TO SEE</summary>\n",
    "\n",
    "First write down what responses you want to keep for each variable in order to answer the question. Then write the code to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baaa926",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7981e9",
   "metadata": {},
   "source": [
    "#### Task 6 Solution\n",
    "\n",
    "To answer the question we need: \n",
    "\n",
    "* Diagnosis only of stroke \n",
    "* All non-aggregate level responses of AgeGroup\n",
    "* Aggregate response All from Sex\n",
    "* Health Boards in the East Region of Scotland\n",
    "* the aggregate level ALL response for Admissions \n",
    "* the Financial Years 2019/20 & 2020/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab97838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stroke_q = stroke.loc[(stroke[\"AdmissionType\"] == \"All\") & \n",
    "                    (stroke[\"Diagnosis\"] == \"Stroke\") & \n",
    "                    (stroke[\"Sex\"] == \"All\") & \n",
    "                    (stroke[\"FinancialYear\"].isin([\"2019/20\", \"2020/21\"])) & \n",
    "                    (~stroke[\"AgeGroup\"].isin([\"All\", \"under75 years\"])) & \n",
    "                    (stroke[\"HBName\"].isin([\"NHS Lothian\", \"NHS Fife\", \"NHS Borders\"]))]\n",
    "\n",
    "stroke_q # now 24 rows of 7 columns \n",
    "## if we did not filter for \"All\" in Sex, we would have 72 rows (duplicate data as there are 3 categories in Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f67f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets check for missing values in HBName and NumberOfDischarges in our now filtered df \n",
    "\n",
    "print(stroke_q.HBName.isna().sum())\n",
    "\n",
    "print(stroke_q.NumberOfDischarges.isna().sum())\n",
    "\n",
    "# happy days, no more missing data to worry about in order to answer the question "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23476c4e",
   "metadata": {},
   "source": [
    "### Task 7 \n",
    "\n",
    "Now that we have our data prepared and check, answer the question posed at the the start of this notebook:\n",
    "    \n",
    "    \n",
    "> What is the average number of discharges with a stroke diagnosis by age group in the East region of Scotland for all admissions in the finanical year 2019/20 and 2020/21?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673eefc",
   "metadata": {},
   "source": [
    "#### Task 7 Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_q.groupby([\"AgeGroup\", \"FinancialYear\", \"HBName\"])[\"NumberOfDischarges\"].mean().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ee6f4",
   "metadata": {},
   "source": [
    "### Task 8 \n",
    "\n",
    "As I mentioned in this week's content, wide data is often more human readable than long data. Take your solution to Task 7 and make the presentation a nicer by reshaping the data a bit! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc63b78",
   "metadata": {},
   "source": [
    "<details><summary style='color:darkblue'>HINT 1: Remember there are multiple functions in Python to reshape data! CLICK HERE TO SEE</summary>\n",
    "\n",
    "Remember that we learned about 4 functions this week to reshape data in Python. \n",
    "    \n",
    "* `melt` to make data longer and its counterpart `stack` for MutiIndex data frames or Series\n",
    "* `pivot` to make data wider and its counterpart `unstack` for MutiIndex data frames or Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a9184",
   "metadata": {},
   "source": [
    "<details><summary style='color:darkblue'>HINT 2: What is the data structure of your solution to Task 7? CLICK HERE TO SEE. </summary>\n",
    "    \n",
    "If you save your solution to task 7 into an object and then run the code `type(object_name)` you will see that the output is not a dataframe but rather a `pandas.core.series.Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c569b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59a98b",
   "metadata": {},
   "source": [
    "#### Task 8 Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24934557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stroke_q.groupby([\"AgeGroup\", \"FinancialYear\", \"HBName\"])[\"NumberOfDischarges\"].mean().dropna().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce959df0",
   "metadata": {},
   "source": [
    "Play around with the `level` argument in `unstack` in the solution above to get a better understanding of how the function works. The default is `level = -1`. For example, you can pass a list to the `level` argument! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420242",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_q.groupby([\"AgeGroup\", \"FinancialYear\", \"HBName\"])[\"NumberOfDischarges\"].mean().dropna().unstack(level = [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdfbf9",
   "metadata": {},
   "source": [
    "---\n",
    "## Well done! 🎉 \n",
    "\n",
    "Well done! You have completed all of the tasks for the Python notebook for this tutorial. If you have not done so yet, now move to the R notebook.\n",
    "\n",
    "Do not forget your 3 stars, a wish, and a step mini-diaries for this week once you have completed the tutorial notebooks and content for the week. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759e793",
   "metadata": {},
   "source": [
    "---\n",
    "*Dr. Brittany Blankinship (2024)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
